hc-nodes:
  hosts:
    # Host1
    host1:

      # VDO config
      # gluster_infra_vdo:
      #    - { name: 'vdo_sdb1', device: '/dev/sdb1', logicalsize: '3000G', emulate512: 'on', slabsize: '32G',
      #       blockmapcachesize:  '128M', readcachesize: '20M', readcache: 'enabled', writepolicy: 'auto' }

      # gluster_infra_pvs: /dev/mapper/vdo_sdb1 # For VDO
      gluster_infra_pvs: /dev/sdb1 # For Non-VDO
      # Stripe unit size always in KiB
      gluster_infra_stripe_unit_size: 256
      gluster_infra_lv_poolmetadatasize: 4G
      gluster_infra_vg_name: gluster_vg_sdb1
      gluster_infra_lv_thinpoolname: gluster_thinpool_sdb1
      #gluster_infra_lv_thinpoolsize: 100G # If not provided entire disk is used

      # Create a thin volume
      gluster_infra_lv_logicalvols:
         - { lvname: 'gluster_lv_data', lvsize: '300G' }
         - { lvname: 'gluster_lv_vmstore', lvsize: '300G' }
         - { lvname: 'gluster_lv_engine', lvsize: '100G' }

      # Mount the devices
      gluster_infra_mount_devices:
         - { path: '/gluster_bricks/data',   lv: gluster_lv_data }
         - { path: '/gluster_bricks/vmstore',   lv: gluster_lv_vmstore }
         - { path: '/gluster_bricks/engine', lv: gluster_lv_engine }

      # Set up GlusterFS hyperconverged interface
      gluster_features_hci_cluster: "{{ groups['hc-nodes'] }}"
      gluster_features_hci_volumes:
         - { volname: 'engine', brick: '/gluster_bricks/engine/engine' }
         - { volname: 'data', brick: '/gluster_bricks/data/data' }
         - { volname: 'vmstore', brick: '/gluster_bricks/vmstore/vmstore' }
    #Host2
    host2:

      # VDO config
      # gluster_infra_vdo:
      #    - { name: 'vdo_sdb1', device: '/dev/sdb1', logicalsize: '3000G', emulate512: 'on', slabsize: '32G',
      #       blockmapcachesize:  '128M', readcachesize: '20M', readcache: 'enabled', writepolicy: 'auto' }

      # gluster_infra_pvs: /dev/mapper/vdo_sdb1 # For VDO
      gluster_infra_pvs: /dev/sdb1 # For Non-VDO
      # Stripe unit size always in KiB
      gluster_infra_stripe_unit_size: 256
      gluster_infra_lv_poolmetadatasize: 4G
      gluster_infra_vg_name: gluster_vg_sdb1
      gluster_infra_lv_thinpoolname: gluster_thinpool_sdb1
      #gluster_infra_lv_thinpoolsize: 100G # If not provided entire disk is used

      # Create a thin volume
      gluster_infra_lv_logicalvols:
         - { lvname: 'gluster_lv_data', lvsize: '300G' }
         - { lvname: 'gluster_lv_vmstore', lvsize: '300G' }
         - { lvname: 'gluster_lv_engine', lvsize: '100G' }

      # Mount the devices
      gluster_infra_mount_devices:
         - { path: '/gluster_bricks/data',   lv: gluster_lv_data }
         - { path: '/gluster_bricks/vmstore',   lv: gluster_lv_vmstore }
         - { path: '/gluster_bricks/engine', lv: gluster_lv_engine }

      # Set up GlusterFS hyperconverged interface
      gluster_features_hci_cluster: "{{ groups['hc-nodes'] }}"
      gluster_features_hci_volumes:
         - { volname: 'engine', brick: '/gluster_bricks/engine/engine' }
         - { volname: 'data', brick: '/gluster_bricks/data/data' }
         - { volname: 'vmstore', brick: '/gluster_bricks/vmstore/vmstore' }
    #Host3
    host3:

      # VDO config
      # gluster_infra_vdo:
      #    - { name: 'vdo_sdb1', device: '/dev/sdb1', logicalsize: '3000G', emulate512: 'on', slabsize: '32G',
      #       blockmapcachesize:  '128M', readcachesize: '20M', readcache: 'enabled', writepolicy: 'auto' }

      # gluster_infra_pvs: /dev/mapper/vdo_sdb1 # For VDO
      gluster_infra_pvs: /dev/sdb1 # For Non-VDO
      # Stripe unit size always in KiB
      gluster_infra_stripe_unit_size: 256
      gluster_infra_lv_poolmetadatasize: 4G
      gluster_infra_vg_name: gluster_vg_sdb1
      gluster_infra_lv_thinpoolname: gluster_thinpool_sdb1
      #gluster_infra_lv_thinpoolsize: 100G # If not provided entire disk is used

      # Create a thin volume
      gluster_infra_lv_logicalvols:
         - { lvname: 'gluster_lv_data', lvsize: '300G' }
         - { lvname: 'gluster_lv_vmstore', lvsize: '300G' }
         - { lvname: 'gluster_lv_engine', lvsize: '100G' }

      # Mount the devices
      gluster_infra_mount_devices:
         - { path: '/gluster_bricks/data',   lv: gluster_lv_data }
         - { path: '/gluster_bricks/vmstore',   lv: gluster_lv_vmstore }
         - { path: '/gluster_bricks/engine', lv: gluster_lv_engine }

      # Set up GlusterFS hyperconverged interface
      gluster_features_hci_cluster: "{{ groups['hc-nodes'] }}"
      gluster_features_hci_volumes:
         - { volname: 'engine', brick: '/gluster_bricks/engine/engine' }
         - { volname: 'data', brick: '/gluster_bricks/data/data' }
         - { volname: 'vmstore', brick: '/gluster_bricks/vmstore/vmstore' }

      # Common configurations
      vars:
        # Firewall setup
        gluster_infra_fw_ports:
           - 2049/tcp
           - 54321/tcp
           - 5900/tcp
           - 5900-6923/tcp
           - 5666/tcp
           - 16514/tcp
        gluster_infra_fw_permanent: true
        gluster_infra_fw_state: enabled
        gluster_infra_fw_zone: public
        gluster_infra_fw_services:
           - glusterfs
        gluster_infra_disktype: RAID6
        gluster_infra_diskcount: 12

## Auto add hosts vars

gluster:
 hosts:
  host2:
  host3:
 vars:
  storage_domains: [{"name":"data","host":"host1","address":"host1","path":"/data","mount_options":"backup-volfile-servers=host2:host3"},{"name":"vmstore","host":"host1","address":"host1","path":"/vmstore","mount_options":"backup-volfile-servers=host2:host3"}]
